\documentclass{article}
\usepackage{geometry}

\usepackage{standalone}
\usepackage{import}

\usepackage{verbatim} % for comments
\usepackage{graphicx} % image
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[colorinlistoftodos]{todonotes} %\todo[inline, color=green!40]{}



\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm, 
 }



\setlength\parindent{0pt} % no indent
\DeclareMathOperator*{\argmax}{argmax} %argmax
\newcommand{\IND}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\TN}{\textnormal}
\newcommand{\argmin}{\TN{argmin}}
\newcommand{\iidsim}{\overset{i.i.d.}{\sim}}
\newcommand{\MC}{\mathcal}
%\newcommand{\define}[1]{\textbf{Define: #1 }}
\newcommand{\lrangle}[1]{\langle #1 \rangle}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\bo}{\textbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\II}{\mathbb{I}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\integer}{\mathbb{Z}}
\newcommand{\by}{\times}

\newcommand{\N}{\MC{N}}
\newcommand{\F}{\MC{F}}
\newcommand{\G}{\MC{G}}
\newcommand{\OLS}{\TN{OLS}}
\newcommand{\betahat}{\hat{\beta}}
\newcommand{\Dto}{\overset{D}{\to}}
\newcommand{\ASto}{\overset{a.s.}{\to}}
\newcommand{\Pto}{\overset{P}{\to}}
\newcommand{\Lto}{\overset{L}{\to}}
\newcommand{\Tr}{\TN{Tr}}
\newcommand{\Var}{\TN{Var}}
\newcommand{\Nstd}{\MC{N}(0, 1)}
\newcommand{\Mult}{\TN{Mult}}
\newcommand{\Pois}{\TN{Pois}}
\newcommand{\Beta}{\TN{Beta}}
\newcommand{\Dir}{\TN{Dir}}
\newcommand{\Bern}{\TN{Bern}}
\newcommand{\Bin}{\TN{Bin}}
\newcommand{\Expo}{\TN{Expo}}
\newcommand{\TNGamma}{\TN{Gamma}}
\newcommand{\Unif}{\tn{Unif}}

\newcommand{\bolda}{\textbf{a}}
\newcommand{\boldb}{\textbf{b}}
\newcommand{\boldc}{\textbf{c}}
\newcommand{\boldd}{\textbf{d}}
\newcommand{\bolde}{\textbf{e}}
\newcommand{\boldf}{\textbf{f}}
\newcommand{\boldg}{\textbf{g}}
\newcommand{\boldh}{\textbf{h}}
\newcommand{\boldi}{\textbf{i}}
\newcommand{\boldj}{\textbf{j}}
\newcommand{\boldk}{\textbf{k}}
\newcommand{\boldl}{\textbf{l}}
\newcommand{\boldm}{\textbf{m}}
\newcommand{\boldn}{\textbf{n}}
\newcommand{\boldo}{\textbf{o}}
\newcommand{\boldp}{\textbf{p}}
\newcommand{\boldq}{\textbf{q}}
\newcommand{\boldr}{\textbf{r}}
\newcommand{\bolds}{\textbf{s}}
\newcommand{\boldt}{\textbf{t}}
\newcommand{\boldu}{\textbf{u}}
\newcommand{\boldv}{\textbf{v}}
\newcommand{\boldw}{\textbf{w}}
\newcommand{\boldx}{\textbf{x}}
\newcommand{\boldy}{\textbf{y}}
\newcommand{\boldz}{\textbf{z}}
\newcommand{\boldA}{\textbf{A}}
\newcommand{\boldB}{\textbf{B}}
\newcommand{\boldC}{\textbf{C}}
\newcommand{\boldD}{\textbf{D}}
\newcommand{\boldE}{\textbf{E}}
\newcommand{\boldF}{\textbf{F}}
\newcommand{\boldG}{\textbf{G}}
\newcommand{\boldH}{\textbf{H}}
\newcommand{\boldI}{\textbf{I}}
\newcommand{\boldJ}{\textbf{J}}
\newcommand{\boldK}{\textbf{K}}
\newcommand{\boldL}{\textbf{L}}
\newcommand{\boldM}{\textbf{M}}
\newcommand{\boldN}{\textbf{N}}
\newcommand{\boldO}{\textbf{O}}
\newcommand{\boldP}{\textbf{P}}
\newcommand{\boldQ}{\textbf{Q}}
\newcommand{\boldR}{\textbf{R}}
\newcommand{\boldS}{\textbf{S}}
\newcommand{\boldT}{\textbf{T}}
\newcommand{\boldU}{\textbf{U}}
\newcommand{\boldV}{\textbf{V}}
\newcommand{\boldW}{\textbf{W}}
\newcommand{\boldX}{\textbf{X}}
\newcommand{\boldY}{\textbf{Y}}
\newcommand{\boldZ}{\textbf{Z}}


\newgeometry{left=1in,right=1in,top=1in,bottom=1in}

\begin{document}
\title{What makes inference for reinforcement learning \\ (adaptively sampled data) challenging?}
\author{Kelly W. Zhang} 
\date{January 2019}
\maketitle

All samples collected by reinforcement learning algorithms are considered \textit{adaptively} chosen.
``Adaptive" data is data that is collected such that the decision about what to sample next depends on the results of previous samples.
For example, in a multi-arm bandit setting, data collected by strategies like $\epsilon$-greedy and Thompson sampling are adaptive, since these strategies are more likely sample arms that gave high rewards in the past than arms that previously gave lower rewards.
Adaptively collected data have a temporal dependency and thus the samples are not independent.
The dependency between samples makes constructing unbiased estimators more difficult for adaptively collected data than for i.i.d data. \\

We describe one illustrative example about how the dependency between adaptively collected samples can lead to biased estimates when using estimators that assume i.i.d. data.
Specifically, we will show how the sample mean of adaptive data can biased \cite{nie2018}. \\

Suppose we have two identical arms that emit rewards that are drawn i.i.d. from a standard normal distribution.
We draw a total of three samples from these arms: $X_1, X_2, X_3$.
$X_1$ is always sampled from arm 1 and $X_2$ is always sampled from arm 2.
We follow a greedy strategy and sample $X_3$ from the arm that emits a higher reward from the first two samples.
Let $M_1$ and $M_2$ represent the sample means of arms 1 and 2 respectively. \\

If $X_1 > X_2$:
$$\begin{cases}
    M_1 = \frac{1}{2} (X_1 + X_3) \\
    M_2 = X_2 \\
\end{cases}$$
else ($X_1 > X_2$):
$$\begin{cases}
    M_1 = X_1 \\
    M_2 = \frac{1}{2} (X_2 + X_3) \\
\end{cases}$$

We let $Z_1, Z_2, Z_3 \iidsim \N(0,1)$ and use the following order statistics notation $Z_{(1)} \sim \min( Z_1, Z_2)$ and $Z_{(2)} \sim \max( Z_1, Z_2)$.

$$M_1 \sim P(X_1 > X_2) \frac{ Z_{(2)} + Z_3 }{2} + P(X_1 < X_2) Z_{(1)}$$

Thus, since by symmetry $P(X_1 > X_2) = P(X_1 < X_2) = \frac{1}{2}$,
$$E[ M_1 ] = P(X_1 > X_2) E[M_1 | X_1 > X_2] + P(X_1 < X_2) E[M_1 | X_1 < X_2]$$
$$= P(X_1 > X_2) E \bigg[ \frac{ Z_{(2)} + Z_3 }{2} \bigg] + P(X_1 < X_2) E[ Z_{(1)} ]$$
$$= \frac{1}{2} E \bigg[ \frac{ Z_{(2)} + Z_3 }{2} \bigg] + \frac{1}{2} E[ Z_{(1)} ]$$
$$= \frac{1}{4} E[ Z_{(2)} ] + \frac{1}{2} E[ Z_{(1)} ] < 0$$
since by symmetry, $E[Z_{(2)}] = -E[Z_{(1)}]$. \\

Thus we have shown in this simple three sample case, that the sample mean is negatively biased when using a greedy strategy.
Note that by symmetry, if we use an ``anti-greedy" strategy of sampling, in which we are more likely to sample the arm that gives a \textit{lower} reward, the expected sample mean \textit{overestimates} the true mean. \\

Nie et al. \cite{nie2018} show that for bandit algorithms that have the properties ``exploit" and ``independence of irrelevant options" (IIO), the sample mean will be negatively biased.
The property exploit is that if arm K is chosen, then in an alternative sample history, if the sample mean of arm K were higher, then arm K would still be chosen.
The IIO property is that if arm K is not chosen, which other arm is selected only depends on the histories of the other arms.

\begin{thebibliography}{9}

\bibitem{deshpande2018}
Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Teddy.
\textit{Accurate Inference in Adaptive Linear Models.}
International Conference on Machine Learning, 2018.

\bibitem{nie2018}
Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou.
\textit{Why Adaptively Collected Data Have Negative Bias and How to Correct for It.}
International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.

\bibitem{laiwei1982}
Tze Leung Lai and Ching Zong Wei.
\textit{Least Squares Estimates in Stochastic Regression Models with Applications to Identification and Control of Dynamic Systems.}
The Annals of Statistics, 1982.

\end{thebibliography}

\end{document}
